[{"title":"Transformer in Convolutional Neural Networks论文阅读笔记","date":"2022-04-23T09:07:45.000Z","path":"posts/9756efb3.html","text":"1.文献出处文献名：Transformer in Convolutional Neural Networks 论文地址:https://arxiv.org/abs/2106.03180 代码地址：https://github.com/yun-liu/TransCNN 2.文献研究内容针对多头自注意力模块（Multi-Head Self-Attention, MHSA）中计算和空间复杂度过高导致Vision Transformer效率低下的缺陷，提出一种层级多头注意力机制模块（Hierarchical Multi-Head Self-Attention, H-MHSA)，最后搭建而成的模型被称为TransCNN。 3.研究创新点 提出了层级多头注意力机制模块（MHSA)来使得Transformer中的自注意力机制计算得更加灵活和高效。 不再计算所有tokens的注意力，而是将patches进一步分组到小网格(grids)中，并计算每个网格中的注意力。这一步捕捉了局部关系，并产生了更具辨别力的局部表示。然后将这些小网格合并成更大的网格，并通过将前一步中的小网格视为标记来计算每个新网格中的注意力。通过这种方式，模型基本上捕获了更大区域中的特征关系。该过程被迭代以逐渐减少tokens的数量。 在整个过程中，H-MHSA模块逐步计算不断增加的区域大小中的自注意力，并自然地以分层方式对全局关系进行建模。 由于每一步的每个网格只有少量的tokens，可以显着降低视觉变换器的计算&#x2F;空间复杂度。 与之前对序列数据进行操作的Transformer网络不同，TransCNN 直接处理 3D 特征图，因此与过去十年提出的先进 CNN 技术兼容。 TransCNN 本质上继承了 CNN 和Transformer的优点，因此在学习尺度&#x2F;移位不变的特征表示和对输入数据中的长期依赖建模方面表现良好。 4.研究思路与方法 提出的TransCNN模型是由层级多头注意力机制模块（H-MHSA)、反向残差瓶颈模块（IRB)和双分支下采样模块（TDB)共同组成的，H-MHSA模块和IRB模块用于获得输入图像得局部与全局特征，TDB模块用于降低特征图的大小。这三个模块可以共同组成一个完整的特征提取网络。 H-MHSA模块、IRB模块和TDB模块的代码实现部分如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110#H-MHSA模块class Attention(nn.Module): def __init__(self, dim, head_dim, grid_size=1, ds_ratio=1, drop=0., norm_layer=nn.BatchNorm2d): super(Attention, self).__init__() assert dim % head_dim == 0 self.num_heads = dim // head_dim self.head_dim = head_dim self.scale = self.head_dim ** -0.5 self.grid_size = grid_size self.norm = norm_layer(dim) self.qkv = nn.Conv2d(dim, dim * 3, 1) self.proj = nn.Conv2d(dim, dim, 1) self.drop = nn.Dropout2d(drop, inplace=True) if grid_size &gt; 1: self.grid_norm = norm_layer(dim) self.avg_pool = nn.AvgPool2d(ds_ratio, stride=ds_ratio) self.ds_norm = norm_layer(dim) self.q = nn.Conv2d(dim, dim, 1) self.kv = nn.Conv2d(dim, dim * 2, 1) def forward(self, x): B, C, H, W = x.shape qkv = self.qkv(self.norm(x)) if self.grid_size &gt; 1: grid_h, grid_w = H // self.grid_size, W // self.grid_size qkv = qkv.reshape(B, 3, self.num_heads, self.head_dim, grid_h, self.grid_size, grid_w, self.grid_size) qkv = qkv.permute(1, 0, 2, 4, 6, 5, 7, 3) qkv = qkv.reshape(3, -1, self.grid_size * self.grid_size, self.head_dim) q, k, v = qkv[0], qkv[1], qkv[2] attn = (q @ k.transpose(-2, -1)) * self.scale attn = attn.softmax(dim=-1) grid_x = (attn @ v).reshape(B, self.num_heads, grid_h, grid_w, self.grid_size, self.grid_size, self.head_dim) grid_x = grid_x.permute(0, 1, 6, 2, 4, 3, 5).reshape(B, C, H, W) grid_x = self.grid_norm(x + grid_x) q = self.q(grid_x).reshape(B, self.num_heads, self.head_dim, -1) q = q.transpose(-2, -1) kv = self.kv(self.ds_norm(self.avg_pool(grid_x))) kv = self.reshape(B, 2, self.num_heads, self.head_dim, -1) kv = kv.permute(1, 0, 2, 4, 3) k, v = kv[0], kv[1] else: qkv = qkv.reshape(B, 3, self.num_heads, self.head_dim, -1) qkv = qkv.permute(1, 0, 2, 4, 3) q, k, v = qkv[0], qkv[1], qkv[2] attn = (q @ k.transpose(-2, -1)) * self.scale attn = attn.softmax(dim=-1) global_x = (attn @ v).transpose(-2, -1).reshape(B, C, H, W) if self.grid_size &gt; 1: global_x = global_x + grid_x x = self.drop(self.proj(global_x)) return x # IRB模块class InvertedResidual(nn.Module): def __init__(self, in_dim, hidden_dim=None, out_dim=None, kernel_size=3, drop=0., act_layer=nn.SiLU, norm_layer=nn.BatchNorm2d): super(InvertedResidual, self).__init__() hidden_dim = hidden_dim or in_dim out_dim = out_dim or in_dim pad = (kernel_size - 1) // 2 self.conv1 = nn.Sequential( nn.Conv2d(in_dim, hidden_dim, kernel_size=1, bias=False), norm_layer(hidden_dim), act_layer(inplace=True) ) self.conv2 = nn.Sequential( nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, padding=pad, groups=hidden_dim, bias=False), norm_layer(hidden_dim), act_layer(inplace=True) ) self.conv3 = nn.Sequential( nn.Conv2d(hidden_dim, out_dim, kernel_size=1, bias=False), norm_layer(out_dim) ) self.drop = nn.Dropout2d(drop, inplace=True) def forward(self, x): x = self.conv1(x) x = self.conv2(x) x = self.drop(x) x = self.conv3(x) x = self.drop(x) return x# 将二者进行组合的模块class Block(nn.Module): def __init__(self, dim, head_dim, grid_size=1, ds_ratio=1, expansion=4, drop=0., drop_path=0., kernel_size=3, act_layer=nn.SiLU, norm_layer=nn.BatchNorm2d): super(Block, self).__init__() self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity() self.attn = Attention(dim, head_dim, grid_size=grid_size, ds_ratio=ds_ratio, drop=drop, norm_layer=norm_layer) self.conv = InvertedResidual(dim, hidden_dim=dim * expansion, out_dim=dim, kernel_size=kernel_size, drop=drop, act_layer=act_layer, norm_layer=norm_layer) def forward(self, x): x = x + self.drop_path(self.attn(x)) x = x + self.drop_path(self.conv(x)) return x 最后是TDB下采样模块。 123456789101112131415class Downsample(nn.Module): def __init__(self, in_dim, out_dim, act_layer=nn.SiLU, norm_layer=nn.BatchNorm2d): super(Downsample).__init__() self.conv = nn.Conv2d(in_dim, out_dim, kernel_size=3, padding=1, stride=2) self.pool = nn.MaxPool2d(2, 2) self.residual = nn.Conv2d(in_dim, out_dim, kernel_size=1) self.norm1 = norm_layer(out_dim) self.norm2 = norm_layer(out_dim) self.act = act_layer(inplace=True) def forward(self, x): x1 = self.norm1(self.conv(x)) x2 = self.norm2(self.residual(self.pool(x))) x = self.act(x1 + x2) return x TransCNN模型设计了两种网络结构，两种结构只在模块堆叠的数量上由区别，整体的参数表如下图所示： 其中C表示通道数，S表示步长，K表示卷积核的大小，E表示IRB模块中的扩展率。在完整的TransCNN网络中，会先使用两个卷积对输入的图像进行降维，获得1&#x2F;4×1&#x2F;4大小的特征图进行后续操作。并在最后的预测时，使用全局平均池化和全连接层来输出可能的类别数，并使用Softmax获得最有可能的预测类别。 5.总结该论文提出了一种结合了CNN和Transformer优势的混合模型TransCNN，并且提出了一个层级多头注意力机制模块（H-MHSA)用于减少原始Transformer中计算注意力机制导致的空间复杂度过高的问题。该模型在多个数据集上进行测试，获得了很好的实验结果。","tags":[{"name":"Transformer CNN","slug":"Transformer-CNN","permalink":"https://zhahoi.com/tags/Transformer-CNN/"}]},{"title":"使用Git上传代码到Github","date":"2022-04-22T06:18:43.000Z","path":"posts/54abec2e.html","text":"1.git全局配置12git config --global user.name &quot;your user name&quot;git config --global user.email &quot;your email&quot; 2.创建新的版本库123456git clone git@***.***.***.gitcd existing_foldertouch README.mdgit add README.mdgit commit -m &quot;add README&quot;git push -u origin main/master 3.针对已经存在的文件夹1234567891011cd existing_folder# 对文件夹进行git初始化git init# 将本地仓库与远方仓库进行连接git remote add origin git@***.***.***.git# 如果远端已经有了文件就先拉一下git pull --rebase origin main# 将修改的文件进行添加git add .git commit -m &quot;Initial commit&quot;git push -u origin main 4.更新已经存在文件夹中的内容1234# 将修改的文件进行添加git add &quot;modify file&quot;git commit -m &quot;Initial commit&quot;git push -u origin main 5.删除Github仓库中的某个文件或者分支 远程删除分支 123456# 查看所有分支git brach -a# 如果需要删除某个分支，就必须先切换到其他的分支上，如切换到master分支上git checkout master# 删除远程分支，如需要删除的远程分支为new_agit push origin --delete new_a 删除文件夹或者仓库 12345678910# 先将远程仓库中的内容拉下来git pull origin main# 查看拉下来有哪些文件dir# 删除某个文件或者文件夹，如targetgit rm -r --cached target # 提交删除说明git commit -m &#x27;delete target file&#x27;# 将修改的内容上传到远端git push -u origin main","tags":[{"name":"git","slug":"git","permalink":"https://zhahoi.com/tags/git/"}]},{"title":"Hello World","date":"2022-04-22T04:32:59.741Z","path":"posts/4a17b156.html","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]