<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>海因斯的部落格</title>
  
  <subtitle>做颗星星，有棱有角，还会发光。</subtitle>
  <link href="https://zhahoi.github.io/atom.xml" rel="self"/>
  
  <link href="https://zhahoi.github.io/"/>
  <updated>2022-04-23T13:15:09.227Z</updated>
  <id>https://zhahoi.github.io/</id>
  
  <author>
    <name>zhahoi</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Transformer in Convolutional Neural Networks论文阅读笔记</title>
    <link href="https://zhahoi.github.io/posts/9756efb3.html"/>
    <id>https://zhahoi.github.io/posts/9756efb3.html</id>
    <published>2022-04-23T09:07:45.000Z</published>
    <updated>2022-04-23T13:15:09.227Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-文献出处"><a href="#1-文献出处" class="headerlink" title="1.文献出处"></a>1.文献出处</h3><p>文献名：<em>Transformer in Convolutional Neural Networks</em></p><p>论文地址:<a href="https://arxiv.org/abs/2106.03180">https://arxiv.org/abs/2106.03180</a> </p><p>代码地址：<a href="https://github.com/yun-liu/TransCNN">https://github.com/yun-liu/TransCNN</a></p></br><h3 id="2-文献研究内容"><a href="#2-文献研究内容" class="headerlink" title="2.文献研究内容"></a>2.文献研究内容</h3><p>针对多头自注意力模块（Multi-Head Self-Attention, MHSA）中计算和空间复杂度过高导致Vision Transformer效率低下的缺陷，提出一种层级多头注意力机制模块（Hierarchical Multi-Head Self-Attention, H-MHSA)，最后搭建而成的模型被称为TransCNN。</p></br><h3 id="3-研究创新点"><a href="#3-研究创新点" class="headerlink" title="3.研究创新点"></a>3.研究创新点</h3><ul><li><p>提出了层级多头注意力机制模块（MHSA)来使得Transformer中的自注意力机制计算得更加灵活和高效。</p></li><li><p>不再计算所有tokens的注意力，而是将patches进一步分组到小网格(grids)中，并计算每个网格中的注意力。这一步捕捉了局部关系，并产生了更具辨别力的局部表示。然后将这些小网格合并成更大的网格，并通过将前一步中的小网格视为标记来计算每个新网格中的注意力。通过这种方式，模型基本上捕获了更大区域中的特征关系。该过程被迭代以逐渐减少tokens的数量。</p></li><li><p>在整个过程中，H-MHSA模块逐步计算不断增加的区域大小中的自注意力，并自然地以分层方式对全局关系进行建模。 由于每一步的每个网格只有少量的tokens，可以显着降低视觉变换器的计算&#x2F;空间复杂度。</p></li><li><p>与之前对序列数据进行操作的Transformer网络不同，TransCNN 直接处理 3D 特征图，因此与过去十年提出的先进 CNN 技术兼容。 TransCNN 本质上继承了 CNN 和Transformer的优点，因此在学习尺度&#x2F;移位不变的特征表示和对输入数据中的长期依赖建模方面表现良好。</p></br></li></ul><h3 id="4-研究思路与方法"><a href="#4-研究思路与方法" class="headerlink" title="4.研究思路与方法"></a>4.研究思路与方法</h3><p><img src="https://s3.bmp.ovh/imgs/2022/04/23/9104476ebb0d9010.png"></p><p>提出的TransCNN模型是由层级多头注意力机制模块（H-MHSA)、反向残差瓶颈模块（IRB)和双分支下采样模块（TDB)共同组成的，H-MHSA模块和IRB模块用于获得输入图像得局部与全局特征，TDB模块用于降低特征图的大小。这三个模块可以共同组成一个完整的特征提取网络。</p><p>H-MHSA模块、IRB模块和TDB模块的代码实现部分如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#H-MHSA模块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, head_dim, grid_size=<span class="number">1</span>, ds_ratio=<span class="number">1</span>, drop=<span class="number">0.</span>, norm_layer=nn.BatchNorm2d</span>):</span><br><span class="line">        <span class="built_in">super</span>(Attention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> dim % head_dim == <span class="number">0</span></span><br><span class="line">        self.num_heads = dim // head_dim</span><br><span class="line">        self.head_dim = head_dim</span><br><span class="line">        self.scale = self.head_dim ** -<span class="number">0.5</span></span><br><span class="line">        self.grid_size = grid_size</span><br><span class="line"></span><br><span class="line">        self.norm = norm_layer(dim)</span><br><span class="line">        self.qkv = nn.Conv2d(dim, dim * <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        self.proj = nn.Conv2d(dim, dim, <span class="number">1</span>)</span><br><span class="line">        self.drop = nn.Dropout2d(drop, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> grid_size &gt; <span class="number">1</span>:</span><br><span class="line">            self.grid_norm = norm_layer(dim)</span><br><span class="line">            self.avg_pool = nn.AvgPool2d(ds_ratio, stride=ds_ratio)</span><br><span class="line">            self.ds_norm = norm_layer(dim)</span><br><span class="line">            self.q = nn.Conv2d(dim, dim, <span class="number">1</span>)</span><br><span class="line">            self.kv = nn.Conv2d(dim, dim * <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        qkv = self.qkv(self.norm(x))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.grid_size &gt; <span class="number">1</span>:</span><br><span class="line">            grid_h, grid_w = H // self.grid_size, W // self.grid_size</span><br><span class="line">            qkv = qkv.reshape(B, <span class="number">3</span>, self.num_heads, self.head_dim, grid_h, self.grid_size, grid_w, self.grid_size)</span><br><span class="line">            qkv = qkv.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">3</span>)</span><br><span class="line">            qkv = qkv.reshape(<span class="number">3</span>, -<span class="number">1</span>, self.grid_size * self.grid_size, self.head_dim)</span><br><span class="line">            q, k, v = qkv[<span class="number">0</span>], qkv[<span class="number">1</span>], qkv[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">            attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) * self.scale</span><br><span class="line">            attn = attn.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">            grid_x = (attn @ v).reshape(B, self.num_heads, grid_h, grid_w, self.grid_size, self.grid_size, self.head_dim)</span><br><span class="line">            grid_x = grid_x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">5</span>).reshape(B, C, H, W)</span><br><span class="line">            grid_x = self.grid_norm(x + grid_x)</span><br><span class="line"></span><br><span class="line">            q = self.q(grid_x).reshape(B, self.num_heads, self.head_dim, -<span class="number">1</span>)</span><br><span class="line">            q = q.transpose(-<span class="number">2</span>, -<span class="number">1</span>)</span><br><span class="line">            kv = self.kv(self.ds_norm(self.avg_pool(grid_x)))</span><br><span class="line">            kv = self.reshape(B, <span class="number">2</span>, self.num_heads, self.head_dim, -<span class="number">1</span>)</span><br><span class="line">            kv = kv.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">            k, v = kv[<span class="number">0</span>], kv[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            qkv = qkv.reshape(B, <span class="number">3</span>, self.num_heads, self.head_dim, -<span class="number">1</span>)</span><br><span class="line">            qkv = qkv.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">            q, k, v = qkv[<span class="number">0</span>], qkv[<span class="number">1</span>], qkv[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) * self.scale</span><br><span class="line">        attn = attn.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        global_x = (attn @ v).transpose(-<span class="number">2</span>, -<span class="number">1</span>).reshape(B, C, H, W)</span><br><span class="line">        <span class="keyword">if</span> self.grid_size &gt; <span class="number">1</span>:</span><br><span class="line">            global_x = global_x + grid_x</span><br><span class="line">        x = self.drop(self.proj(global_x))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">      </span><br><span class="line"><span class="comment"># IRB模块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim, hidden_dim=<span class="literal">None</span>, out_dim=<span class="literal">None</span>, kernel_size=<span class="number">3</span>,</span></span><br><span class="line"><span class="params">                 drop=<span class="number">0.</span>, act_layer=nn.SiLU, norm_layer=nn.BatchNorm2d</span>):</span><br><span class="line">        <span class="built_in">super</span>(InvertedResidual, self).__init__()</span><br><span class="line">        hidden_dim = hidden_dim <span class="keyword">or</span> in_dim</span><br><span class="line">        out_dim = out_dim <span class="keyword">or</span> in_dim</span><br><span class="line">        pad = (kernel_size - <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_dim, hidden_dim, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            norm_layer(hidden_dim),</span><br><span class="line">            act_layer(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        self.conv2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, padding=pad, groups=hidden_dim, bias=<span class="literal">False</span>),</span><br><span class="line">            norm_layer(hidden_dim),</span><br><span class="line">            act_layer(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        self.conv3 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(hidden_dim, out_dim, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            norm_layer(out_dim)</span><br><span class="line">        )</span><br><span class="line">        self.drop = nn.Dropout2d(drop, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.drop(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.drop(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将二者进行组合的模块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, head_dim, grid_size=<span class="number">1</span>, ds_ratio=<span class="number">1</span>, expansion=<span class="number">4</span>,</span></span><br><span class="line"><span class="params">                 drop=<span class="number">0.</span>, drop_path=<span class="number">0.</span>, kernel_size=<span class="number">3</span>, act_layer=nn.SiLU,</span></span><br><span class="line"><span class="params">                 norm_layer=nn.BatchNorm2d</span>):</span><br><span class="line">        <span class="built_in">super</span>(Block, self).__init__()</span><br><span class="line">        self.drop_path = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.attn = Attention(dim, head_dim, grid_size=grid_size, ds_ratio=ds_ratio,</span><br><span class="line">            drop=drop, norm_layer=norm_layer)</span><br><span class="line">        self.conv = InvertedResidual(dim, hidden_dim=dim * expansion, out_dim=dim,</span><br><span class="line">            kernel_size=kernel_size, drop=drop, act_layer=act_layer, norm_layer=norm_layer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + self.drop_path(self.attn(x))</span><br><span class="line">        x = x + self.drop_path(self.conv(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>最后是TDB下采样模块。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Downsample</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim, out_dim, act_layer=nn.SiLU, norm_layer=nn.BatchNorm2d</span>):</span><br><span class="line">        <span class="built_in">super</span>(Downsample).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_dim, out_dim, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.residual = nn.Conv2d(in_dim, out_dim, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.norm1 = norm_layer(out_dim)</span><br><span class="line">        self.norm2 = norm_layer(out_dim)</span><br><span class="line">        self.act = act_layer(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x1 = self.norm1(self.conv(x))</span><br><span class="line">        x2 = self.norm2(self.residual(self.pool(x)))</span><br><span class="line">        x = self.act(x1 + x2)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></br><p>TransCNN模型设计了两种网络结构，两种结构只在模块堆叠的数量上由区别，整体的参数表如下图所示：</p><p><a href="https://imgtu.com/i/LfywTO"><img src="https://s1.ax1x.com/2022/04/23/LfywTO.png" alt="LfywTO.png"></a></p><p>其中<code>C</code>表示通道数，<code>S</code>表示步长，<code>K</code>表示卷积核的大小，<code>E</code>表示IRB模块中的扩展率。在完整的TransCNN网络中，会先使用两个卷积对输入的图像进行降维，获得1&#x2F;4×1&#x2F;4大小的特征图进行后续操作。并在最后的预测时，使用全局平均池化和全连接层来输出可能的类别数，并使用Softmax获得最有可能的预测类别。</p></br><h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h3><p>该论文提出了一种结合了CNN和Transformer优势的混合模型TransCNN，并且提出了一个层级多头注意力机制模块（H-MHSA)用于减少原始Transformer中计算注意力机制导致的空间复杂度过高的问题。该模型在多个数据集上进行测试，获得了很好的实验结果。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;1-文献出处&quot;&gt;&lt;a href=&quot;#1-文献出处&quot; class=&quot;headerlink&quot; title=&quot;1.文献出处&quot;&gt;&lt;/a&gt;1.文献出处&lt;/h3&gt;&lt;p&gt;文献名：&lt;em&gt;Transformer in Convolutional Neural Networks&lt;/</summary>
      
    
    
    
    <category term="论文阅读" scheme="https://zhahoi.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Transformer CNN" scheme="https://zhahoi.github.io/tags/Transformer-CNN/"/>
    
  </entry>
  
  <entry>
    <title>使用Git上传代码到Github</title>
    <link href="https://zhahoi.github.io/posts/54abec2e.html"/>
    <id>https://zhahoi.github.io/posts/54abec2e.html</id>
    <published>2022-04-22T06:18:43.000Z</published>
    <updated>2022-04-23T12:13:03.091Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-git全局配置"><a href="#1-git全局配置" class="headerlink" title="1.git全局配置"></a>1.git全局配置</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;your user name&quot;</span><br><span class="line">git config --global user.email &quot;your email&quot;</span><br></pre></td></tr></table></figure><h3 id="2-创建新的版本库"><a href="#2-创建新的版本库" class="headerlink" title="2.创建新的版本库"></a>2.创建新的版本库</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git clone git@***.***.***.git</span><br><span class="line">cd existing_folder</span><br><span class="line">touch README.md</span><br><span class="line">git add README.md</span><br><span class="line">git commit -m &quot;add README&quot;</span><br><span class="line">git push -u origin main/master</span><br></pre></td></tr></table></figure><h3 id="3-针对已经存在的文件夹"><a href="#3-针对已经存在的文件夹" class="headerlink" title="3.针对已经存在的文件夹"></a>3.针对已经存在的文件夹</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cd existing_folder</span><br><span class="line"># 对文件夹进行git初始化</span><br><span class="line">git init</span><br><span class="line"># 将本地仓库与远方仓库进行连接</span><br><span class="line">git remote add origin git@***.***.***.git</span><br><span class="line"># 如果远端已经有了文件就先拉一下</span><br><span class="line">git pull --rebase origin main</span><br><span class="line"># 将修改的文件进行添加</span><br><span class="line">git add .</span><br><span class="line">git commit -m &quot;Initial commit&quot;</span><br><span class="line">git push -u origin main</span><br></pre></td></tr></table></figure><h3 id="4-更新已经存在文件夹中的内容"><a href="#4-更新已经存在文件夹中的内容" class="headerlink" title="4.更新已经存在文件夹中的内容"></a>4.更新已经存在文件夹中的内容</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 将修改的文件进行添加</span><br><span class="line">git add &quot;modify file&quot;</span><br><span class="line">git commit -m &quot;Initial commit&quot;</span><br><span class="line">git push -u origin main</span><br></pre></td></tr></table></figure><h3 id="5-删除Github仓库中的某个文件或者分支"><a href="#5-删除Github仓库中的某个文件或者分支" class="headerlink" title="5.删除Github仓库中的某个文件或者分支"></a>5.删除Github仓库中的某个文件或者分支</h3><ul><li>远程删除分支</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 查看所有分支</span><br><span class="line">git brach -a</span><br><span class="line"># 如果需要删除某个分支，就必须先切换到其他的分支上，如切换到master分支上</span><br><span class="line">git checkout master</span><br><span class="line"># 删除远程分支，如需要删除的远程分支为new_a</span><br><span class="line">git push origin --delete new_a</span><br></pre></td></tr></table></figure><ul><li>删除文件夹或者仓库</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 先将远程仓库中的内容拉下来</span><br><span class="line">git pull origin main</span><br><span class="line"># 查看拉下来有哪些文件</span><br><span class="line">dir</span><br><span class="line"># 删除某个文件或者文件夹，如target</span><br><span class="line">git rm -r --cached target </span><br><span class="line"># 提交删除说明</span><br><span class="line">git commit -m &#x27;delete target file&#x27;</span><br><span class="line"># 将修改的内容上传到远端</span><br><span class="line">git push -u origin main</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;1-git全局配置&quot;&gt;&lt;a href=&quot;#1-git全局配置&quot; class=&quot;headerlink&quot; title=&quot;1.git全局配置&quot;&gt;&lt;/a&gt;1.git全局配置&lt;/h3&gt;&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;t</summary>
      
    
    
    
    <category term="常用指令" scheme="https://zhahoi.github.io/categories/%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/"/>
    
    
    <category term="git" scheme="https://zhahoi.github.io/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://zhahoi.github.io/posts/4a17b156.html"/>
    <id>https://zhahoi.github.io/posts/4a17b156.html</id>
    <published>2022-04-22T04:32:59.741Z</published>
    <updated>2022-04-22T05:33:52.726Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
