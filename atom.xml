<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>海因斯的部落格</title>
  
  <subtitle>做颗星星，有棱有角，还会发光。</subtitle>
  <link href="https://zhahoi.github.io/atom.xml" rel="self"/>
  
  <link href="https://zhahoi.github.io/"/>
  <updated>2022-04-23T11:48:08.743Z</updated>
  <id>https://zhahoi.github.io/</id>
  
  <author>
    <name>zhahoi</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Transformer in Convolutional Neural Networks论文阅读笔记</title>
    <link href="https://zhahoi.github.io/posts/9756efb3.html"/>
    <id>https://zhahoi.github.io/posts/9756efb3.html</id>
    <published>2022-04-23T09:07:45.000Z</published>
    <updated>2022-04-23T11:48:08.743Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-文献出处"><a href="#1-文献出处" class="headerlink" title="1.文献出处"></a>1.文献出处</h3><p>文献名：<em>Transformer in Convolutional Neural Networks</em></p><p>论文地址:<a href="https://arxiv.org/abs/2106.03180" target="_blank" rel="noopener">https://arxiv.org/abs/2106.03180</a> </p><p>代码地址：<a href="https://github.com/yun-liu/TransCNN" target="_blank" rel="noopener">https://github.com/yun-liu/TransCNN</a></p></br><h3 id="2-文献研究内容"><a href="#2-文献研究内容" class="headerlink" title="2.文献研究内容"></a>2.文献研究内容</h3><p>针对多头自注意力模块（Multi-Head Self-Attention, MHSA）中计算和空间复杂度过高导致Vision Transformer效率低下的缺陷，提出一种层级多头注意力机制模块（Hierarchical Multi-Head Self-Attention, H-MHSA)，最后搭建而成的模型被称为TransCNN。</p></br><h3 id="3-研究创新点"><a href="#3-研究创新点" class="headerlink" title="3.研究创新点"></a>3.研究创新点</h3><ul><li><p>提出了层级多头注意力机制模块（MHSA)来使得Transformer中的自注意力机制计算得更加灵活和高效。</p></li><li><p>不再计算所有tokens的注意力，而是将patches进一步分组到小网格(grids)中，并计算每个网格中的注意力。这一步捕捉了局部关系，并产生了更具辨别力的局部表示。然后将这些小网格合并成更大的网格，并通过将前一步中的小网格视为标记来计算每个新网格中的注意力。通过这种方式，模型基本上捕获了更大区域中的特征关系。该过程被迭代以逐渐减少tokens的数量。</p></li><li><p>在整个过程中，H-MHSA模块逐步计算不断增加的区域大小中的自注意力，并自然地以分层方式对全局关系进行建模。 由于每一步的每个网格只有少量的tokens，可以显着降低视觉变换器的计算&#x2F;空间复杂度。</p></li><li><p>与之前对序列数据进行操作的Transformer网络不同，TransCNN 直接处理 3D 特征图，因此与过去十年提出的先进 CNN 技术兼容。 TransCNN 本质上继承了 CNN 和Transformer的优点，因此在学习尺度&#x2F;移位不变的特征表示和对输入数据中的长期依赖建模方面表现良好。</p></br></li></ul><h3 id="4-研究思路与方法"><a href="#4-研究思路与方法" class="headerlink" title="4.研究思路与方法"></a>4.研究思路与方法</h3><p><img src="https://s3.bmp.ovh/imgs/2022/04/23/9104476ebb0d9010.png"></p><p>提出的TransCNN模型是由层级多头注意力机制模块（H-MHSA)、反向残差瓶颈模块（IRB)和双分支下采样模块（TDB)共同组成的，H-MHSA模块和IRB模块用于获得输入图像得局部与全局特征，TDB模块用于降低特征图的大小。这三个模块可以共同组成一个完整的特征提取网络。</p><p>H-MHSA模块、IRB模块和TDB模块的代码实现部分如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line">#H-MHSA模块</span><br><span class="line">class Attention(nn.Module):</span><br><span class="line">    def __init__(self, dim, head_dim, grid_size&#x3D;1, ds_ratio&#x3D;1, drop&#x3D;0., norm_layer&#x3D;nn.BatchNorm2d):</span><br><span class="line">        super(Attention, self).__init__()</span><br><span class="line">        assert dim % head_dim &#x3D;&#x3D; 0</span><br><span class="line">        self.num_heads &#x3D; dim &#x2F;&#x2F; head_dim</span><br><span class="line">        self.head_dim &#x3D; head_dim</span><br><span class="line">        self.scale &#x3D; self.head_dim ** -0.5</span><br><span class="line">        self.grid_size &#x3D; grid_size</span><br><span class="line"></span><br><span class="line">        self.norm &#x3D; norm_layer(dim)</span><br><span class="line">        self.qkv &#x3D; nn.Conv2d(dim, dim * 3, 1)</span><br><span class="line">        self.proj &#x3D; nn.Conv2d(dim, dim, 1)</span><br><span class="line">        self.drop &#x3D; nn.Dropout2d(drop, inplace&#x3D;True)</span><br><span class="line"></span><br><span class="line">        if grid_size &gt; 1:</span><br><span class="line">            self.grid_norm &#x3D; norm_layer(dim)</span><br><span class="line">            self.avg_pool &#x3D; nn.AvgPool2d(ds_ratio, stride&#x3D;ds_ratio)</span><br><span class="line">            self.ds_norm &#x3D; norm_layer(dim)</span><br><span class="line">            self.q &#x3D; nn.Conv2d(dim, dim, 1)</span><br><span class="line">            self.kv &#x3D; nn.Conv2d(dim, dim * 2, 1)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        B, C, H, W &#x3D; x.shape</span><br><span class="line">        qkv &#x3D; self.qkv(self.norm(x))</span><br><span class="line"></span><br><span class="line">        if self.grid_size &gt; 1:</span><br><span class="line">            grid_h, grid_w &#x3D; H &#x2F;&#x2F; self.grid_size, W &#x2F;&#x2F; self.grid_size</span><br><span class="line">            qkv &#x3D; qkv.reshape(B, 3, self.num_heads, self.head_dim, grid_h, self.grid_size, grid_w, self.grid_size)</span><br><span class="line">            qkv &#x3D; qkv.permute(1, 0, 2, 4, 6, 5, 7, 3)</span><br><span class="line">            qkv &#x3D; qkv.reshape(3, -1, self.grid_size * self.grid_size, self.head_dim)</span><br><span class="line">            q, k, v &#x3D; qkv[0], qkv[1], qkv[2]</span><br><span class="line"></span><br><span class="line">            attn &#x3D; (q @ k.transpose(-2, -1)) * self.scale</span><br><span class="line">            attn &#x3D; attn.softmax(dim&#x3D;-1)</span><br><span class="line">            grid_x &#x3D; (attn @ v).reshape(B, self.num_heads, grid_h, grid_w, self.grid_size, self.grid_size, self.head_dim)</span><br><span class="line">            grid_x &#x3D; grid_x.permute(0, 1, 6, 2, 4, 3, 5).reshape(B, C, H, W)</span><br><span class="line">            grid_x &#x3D; self.grid_norm(x + grid_x)</span><br><span class="line"></span><br><span class="line">            q &#x3D; self.q(grid_x).reshape(B, self.num_heads, self.head_dim, -1)</span><br><span class="line">            q &#x3D; q.transpose(-2, -1)</span><br><span class="line">            kv &#x3D; self.kv(self.ds_norm(self.avg_pool(grid_x)))</span><br><span class="line">            kv &#x3D; self.reshape(B, 2, self.num_heads, self.head_dim, -1)</span><br><span class="line">            kv &#x3D; kv.permute(1, 0, 2, 4, 3)</span><br><span class="line">            k, v &#x3D; kv[0], kv[1]</span><br><span class="line"></span><br><span class="line">        else:</span><br><span class="line">            qkv &#x3D; qkv.reshape(B, 3, self.num_heads, self.head_dim, -1)</span><br><span class="line">            qkv &#x3D; qkv.permute(1, 0, 2, 4, 3)</span><br><span class="line">            q, k, v &#x3D; qkv[0], qkv[1], qkv[2]</span><br><span class="line"></span><br><span class="line">        attn &#x3D; (q @ k.transpose(-2, -1)) * self.scale</span><br><span class="line">        attn &#x3D; attn.softmax(dim&#x3D;-1)</span><br><span class="line">        global_x &#x3D; (attn @ v).transpose(-2, -1).reshape(B, C, H, W)</span><br><span class="line">        if self.grid_size &gt; 1:</span><br><span class="line">            global_x &#x3D; global_x + grid_x</span><br><span class="line">        x &#x3D; self.drop(self.proj(global_x))</span><br><span class="line"></span><br><span class="line">        return x</span><br><span class="line">      </span><br><span class="line"># IRB模块</span><br><span class="line">class InvertedResidual(nn.Module):</span><br><span class="line">    def __init__(self, in_dim, hidden_dim&#x3D;None, out_dim&#x3D;None, kernel_size&#x3D;3,</span><br><span class="line">                 drop&#x3D;0., act_layer&#x3D;nn.SiLU, norm_layer&#x3D;nn.BatchNorm2d):</span><br><span class="line">        super(InvertedResidual, self).__init__()</span><br><span class="line">        hidden_dim &#x3D; hidden_dim or in_dim</span><br><span class="line">        out_dim &#x3D; out_dim or in_dim</span><br><span class="line">        pad &#x3D; (kernel_size - 1) &#x2F;&#x2F; 2</span><br><span class="line"></span><br><span class="line">        self.conv1 &#x3D; nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_dim, hidden_dim, kernel_size&#x3D;1, bias&#x3D;False),</span><br><span class="line">            norm_layer(hidden_dim),</span><br><span class="line">            act_layer(inplace&#x3D;True)</span><br><span class="line">        )</span><br><span class="line">        self.conv2 &#x3D; nn.Sequential(</span><br><span class="line">            nn.Conv2d(hidden_dim, hidden_dim, kernel_size&#x3D;kernel_size, padding&#x3D;pad, groups&#x3D;hidden_dim, bias&#x3D;False),</span><br><span class="line">            norm_layer(hidden_dim),</span><br><span class="line">            act_layer(inplace&#x3D;True)</span><br><span class="line">        )</span><br><span class="line">        self.conv3 &#x3D; nn.Sequential(</span><br><span class="line">            nn.Conv2d(hidden_dim, out_dim, kernel_size&#x3D;1, bias&#x3D;False),</span><br><span class="line">            norm_layer(out_dim)</span><br><span class="line">        )</span><br><span class="line">        self.drop &#x3D; nn.Dropout2d(drop, inplace&#x3D;True)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x &#x3D; self.conv1(x)</span><br><span class="line">        x &#x3D; self.conv2(x)</span><br><span class="line">        x &#x3D; self.drop(x)</span><br><span class="line">        x &#x3D; self.conv3(x)</span><br><span class="line">        x &#x3D; self.drop(x)</span><br><span class="line"></span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"># 将二者进行组合的模块</span><br><span class="line">class Block(nn.Module):</span><br><span class="line">    def __init__(self, dim, head_dim, grid_size&#x3D;1, ds_ratio&#x3D;1, expansion&#x3D;4,</span><br><span class="line">                 drop&#x3D;0., drop_path&#x3D;0., kernel_size&#x3D;3, act_layer&#x3D;nn.SiLU,</span><br><span class="line">                 norm_layer&#x3D;nn.BatchNorm2d):</span><br><span class="line">        super(Block, self).__init__()</span><br><span class="line">        self.drop_path &#x3D; DropPath(drop_path) if drop_path &gt; 0. else nn.Identity()</span><br><span class="line">        self.attn &#x3D; Attention(dim, head_dim, grid_size&#x3D;grid_size, ds_ratio&#x3D;ds_ratio,</span><br><span class="line">            drop&#x3D;drop, norm_layer&#x3D;norm_layer)</span><br><span class="line">        self.conv &#x3D; InvertedResidual(dim, hidden_dim&#x3D;dim * expansion, out_dim&#x3D;dim,</span><br><span class="line">            kernel_size&#x3D;kernel_size, drop&#x3D;drop, act_layer&#x3D;act_layer, norm_layer&#x3D;norm_layer)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x &#x3D; x + self.drop_path(self.attn(x))</span><br><span class="line">        x &#x3D; x + self.drop_path(self.conv(x))</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure><p>最后是TDB下采样模块。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class Downsample(nn.Module):</span><br><span class="line">    def __init__(self, in_dim, out_dim, act_layer&#x3D;nn.SiLU, norm_layer&#x3D;nn.BatchNorm2d):</span><br><span class="line">        super(Downsample).__init__()</span><br><span class="line">        self.conv &#x3D; nn.Conv2d(in_dim, out_dim, kernel_size&#x3D;3, padding&#x3D;1, stride&#x3D;2)</span><br><span class="line">        self.pool &#x3D; nn.MaxPool2d(2, 2)</span><br><span class="line">        self.residual &#x3D; nn.Conv2d(in_dim, out_dim, kernel_size&#x3D;1)</span><br><span class="line">        self.norm1 &#x3D; norm_layer(out_dim)</span><br><span class="line">        self.norm2 &#x3D; norm_layer(out_dim)</span><br><span class="line">        self.act &#x3D; act_layer(inplace&#x3D;True)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x1 &#x3D; self.norm1(self.conv(x))</span><br><span class="line">        x2 &#x3D; self.norm2(self.residual(self.pool(x)))</span><br><span class="line">        x &#x3D; self.act(x1 + x2)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure></br><p>TransCNN模型设计了两种网络结构，两种结构只在模块堆叠的数量上由区别，整体的参数表如下图所示：</p><p><a href="https://imgtu.com/i/LfywTO" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2022/04/23/LfywTO.png" alt="LfywTO.png"></a></p><p>其中<code>C</code>表示通道数，<code>S</code>表示步长，<code>K</code>表示卷积核的大小，<code>E</code>表示IRB模块中的扩展率。在完整的TransCNN网络中，会先使用两个卷积对输入的图像进行降维，获得1&#x2F;4×1&#x2F;4大小的特征图进行后续操作。并在最后的预测时，使用全局平均池化和全连接层来输出可能的类别数，并使用Softmax获得最有可能的预测类别。</p></br><h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h3><p>该论文提出了一种结合了CNN和Transformer优势的混合模型TransCNN，并且提出了一个层级多头注意力机制模块（H-MHSA)用于减少原始Transformer中计算注意力机制导致的空间复杂度过高的问题。该模型在多个数据集上进行测试，获得了很好的实验结果。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;1-文献出处&quot;&gt;&lt;a href=&quot;#1-文献出处&quot; class=&quot;headerlink&quot; title=&quot;1.文献出处&quot;&gt;&lt;/a&gt;1.文献出处&lt;/h3&gt;&lt;p&gt;文献名：&lt;em&gt;Transformer in Convolutional Neural Networks&lt;/</summary>
      
    
    
    
    <category term="论文阅读" scheme="https://zhahoi.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Transformer CNN" scheme="https://zhahoi.github.io/tags/Transformer-CNN/"/>
    
  </entry>
  
  <entry>
    <title>使用Git上传代码到Github</title>
    <link href="https://zhahoi.github.io/posts/54abec2e.html"/>
    <id>https://zhahoi.github.io/posts/54abec2e.html</id>
    <published>2022-04-22T06:18:43.000Z</published>
    <updated>2022-04-22T06:20:48.978Z</updated>
    
    <content type="html"><![CDATA[<h4 id="1-git全局配置"><a href="#1-git全局配置" class="headerlink" title="1.git全局配置"></a>1.git全局配置</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git<span class="built_in"> config </span>--global user.name <span class="string">"your user name"</span></span><br><span class="line">git<span class="built_in"> config </span>--global user.email <span class="string">"your email"</span></span><br></pre></td></tr></table></figure><h4 id="2-创建新的版本库"><a href="#2-创建新的版本库" class="headerlink" title="2.创建新的版本库"></a>2.创建新的版本库</h4><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">git</span> <span class="string">clone git@***.***.***.git</span></span><br><span class="line"><span class="attr">cd</span> <span class="string">existing_folder</span></span><br><span class="line"><span class="attr">touch</span> <span class="string">README.md</span></span><br><span class="line"><span class="attr">git</span> <span class="string">add README.md</span></span><br><span class="line"><span class="attr">git</span> <span class="string">commit -m "add README"</span></span><br><span class="line"><span class="attr">git</span> <span class="string">push -u origin main/master</span></span><br></pre></td></tr></table></figure><h4 id="3-针对已经存在的文件夹"><a href="#3-针对已经存在的文件夹" class="headerlink" title="3.针对已经存在的文件夹"></a>3.针对已经存在的文件夹</h4><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">cd</span> <span class="string">existing_folder</span></span><br><span class="line"><span class="comment"># 对文件夹进行git初始化</span></span><br><span class="line"><span class="attr">git</span> <span class="string">init</span></span><br><span class="line"><span class="comment"># 将本地仓库与远方仓库进行连接</span></span><br><span class="line"><span class="attr">git</span> <span class="string">remote add origin git@***.***.***.git</span></span><br><span class="line"><span class="comment"># 如果远端已经有了文件就先拉一下</span></span><br><span class="line"><span class="attr">git</span> <span class="string">pull --rebase origin main</span></span><br><span class="line"><span class="comment"># 将修改的文件进行添加</span></span><br><span class="line"><span class="attr">git</span> <span class="string">add .</span></span><br><span class="line"><span class="attr">git</span> <span class="string">commit -m "Initial commit"</span></span><br><span class="line"><span class="attr">git</span> <span class="string">push -u origin main</span></span><br></pre></td></tr></table></figure><h4 id="4-更新已经存在文件夹中的内容"><a href="#4-更新已经存在文件夹中的内容" class="headerlink" title="4.更新已经存在文件夹中的内容"></a>4.更新已经存在文件夹中的内容</h4><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># 将修改的文件进行添加</span></span><br><span class="line">git <span class="keyword">add</span> <span class="string">"modify file"</span></span><br><span class="line">git commit -m <span class="string">"Initial commit"</span></span><br><span class="line">git <span class="keyword">push</span> -u origin main</span><br></pre></td></tr></table></figure><h4 id="5-删除Github仓库中的某个文件或者分支"><a href="#5-删除Github仓库中的某个文件或者分支" class="headerlink" title="5.删除Github仓库中的某个文件或者分支"></a>5.删除Github仓库中的某个文件或者分支</h4><ul><li>远程删除分支</li></ul><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看所有分支</span></span><br><span class="line"><span class="attr">git</span> <span class="string">brach -a</span></span><br><span class="line"><span class="comment"># 如果需要删除某个分支，就必须先切换到其他的分支上，如切换到master分支上</span></span><br><span class="line"><span class="attr">git</span> <span class="string">checkout master</span></span><br><span class="line"><span class="comment"># 删除远程分支，如需要删除的远程分支为new_a</span></span><br><span class="line"><span class="attr">git</span> <span class="string">push origin --delete new_a</span></span><br></pre></td></tr></table></figure><ul><li>删除文件夹或者仓库</li></ul><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># 先将远程仓库中的内容拉下来</span></span><br><span class="line">git pull origin main</span><br><span class="line"><span class="meta"># 查看拉下来有哪些文件</span></span><br><span class="line">dir</span><br><span class="line"><span class="meta"># 删除某个文件或者文件夹，如target</span></span><br><span class="line">git rm -r --cached target </span><br><span class="line"><span class="meta"># 提交删除说明</span></span><br><span class="line">git commit -m <span class="string">'delete target file'</span></span><br><span class="line"><span class="meta"># 将修改的内容上传到远端</span></span><br><span class="line">git push -u origin main</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h4 id=&quot;1-git全局配置&quot;&gt;&lt;a href=&quot;#1-git全局配置&quot; class=&quot;headerlink&quot; title=&quot;1.git全局配置&quot;&gt;&lt;/a&gt;1.git全局配置&lt;/h4&gt;&lt;figure class=&quot;highlight routeros&quot;&gt;&lt;table&gt;&lt;tr</summary>
      
    
    
    
    <category term="常用指令" scheme="https://zhahoi.github.io/categories/%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/"/>
    
    
    <category term="git" scheme="https://zhahoi.github.io/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://zhahoi.github.io/posts/4a17b156.html"/>
    <id>https://zhahoi.github.io/posts/4a17b156.html</id>
    <published>2022-04-22T04:32:59.741Z</published>
    <updated>2022-04-22T05:33:52.726Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.</summary>
      
    
    
    
    
  </entry>
  
</feed>
