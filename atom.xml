<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>海因斯的部落格</title>
  
  <subtitle>做颗星星，有棱有角，还会发光。</subtitle>
  <link href="https://zhahoi.github.io/atom.xml" rel="self"/>
  
  <link href="https://zhahoi.github.io/"/>
  <updated>2022-04-23T12:54:34.547Z</updated>
  <id>https://zhahoi.github.io/</id>
  
  <author>
    <name>zhahoi</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Transformer in Convolutional Neural Networks论文阅读笔记</title>
    <link href="https://zhahoi.github.io/posts/9756efb3.html"/>
    <id>https://zhahoi.github.io/posts/9756efb3.html</id>
    <published>2022-04-23T09:07:45.000Z</published>
    <updated>2022-04-23T12:54:34.547Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-文献出处"><a href="#1-文献出处" class="headerlink" title="1.文献出处"></a>1.文献出处</h3><p>文献名：<em>Transformer in Convolutional Neural Networks</em></p><p>论文地址:<a href="https://arxiv.org/abs/2106.03180">https://arxiv.org/abs/2106.03180</a> </p><p>代码地址：<a href="https://github.com/yun-liu/TransCNN">https://github.com/yun-liu/TransCNN</a></p></br><h3 id="2-文献研究内容"><a href="#2-文献研究内容" class="headerlink" title="2.文献研究内容"></a>2.文献研究内容</h3><p>针对多头自注意力模块（Multi-Head Self-Attention, MHSA）中计算和空间复杂度过高导致Vision Transformer效率低下的缺陷，提出一种层级多头注意力机制模块（Hierarchical Multi-Head Self-Attention, H-MHSA)，最后搭建而成的模型被称为TransCNN。</p></br><h3 id="3-研究创新点"><a href="#3-研究创新点" class="headerlink" title="3.研究创新点"></a>3.研究创新点</h3><ul><li><p>提出了层级多头注意力机制模块（MHSA)来使得Transformer中的自注意力机制计算得更加灵活和高效。</p></li><li><p>不再计算所有tokens的注意力，而是将patches进一步分组到小网格(grids)中，并计算每个网格中的注意力。这一步捕捉了局部关系，并产生了更具辨别力的局部表示。然后将这些小网格合并成更大的网格，并通过将前一步中的小网格视为标记来计算每个新网格中的注意力。通过这种方式，模型基本上捕获了更大区域中的特征关系。该过程被迭代以逐渐减少tokens的数量。</p></li><li><p>在整个过程中，H-MHSA模块逐步计算不断增加的区域大小中的自注意力，并自然地以分层方式对全局关系进行建模。 由于每一步的每个网格只有少量的tokens，可以显着降低视觉变换器的计算&#x2F;空间复杂度。</p></li><li><p>与之前对序列数据进行操作的Transformer网络不同，TransCNN 直接处理 3D 特征图，因此与过去十年提出的先进 CNN 技术兼容。 TransCNN 本质上继承了 CNN 和Transformer的优点，因此在学习尺度&#x2F;移位不变的特征表示和对输入数据中的长期依赖建模方面表现良好。</p></br></li></ul><h3 id="4-研究思路与方法"><a href="#4-研究思路与方法" class="headerlink" title="4.研究思路与方法"></a>4.研究思路与方法</h3><p><img src="https://s3.bmp.ovh/imgs/2022/04/23/9104476ebb0d9010.png"></p><p>提出的TransCNN模型是由层级多头注意力机制模块（H-MHSA)、反向残差瓶颈模块（IRB)和双分支下采样模块（TDB)共同组成的，H-MHSA模块和IRB模块用于获得输入图像得局部与全局特征，TDB模块用于降低特征图的大小。这三个模块可以共同组成一个完整的特征提取网络。</p><p>H-MHSA模块、IRB模块和TDB模块的代码实现部分如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#H-MHSA模块</span><span class="token keyword">class</span> <span class="token class-name">Attention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> head_dim<span class="token punctuation">,</span> grid_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ds_ratio<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> drop<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Attention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> dim <span class="token operator">%</span> head_dim <span class="token operator">==</span> <span class="token number">0</span>        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> dim <span class="token operator">//</span> head_dim        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> head_dim        self<span class="token punctuation">.</span>scale <span class="token operator">=</span> self<span class="token punctuation">.</span>head_dim <span class="token operator">**</span> <span class="token operator">-</span><span class="token number">0.5</span>        self<span class="token punctuation">.</span>grid_size <span class="token operator">=</span> grid_size        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span>dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>qkv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> dim <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout2d<span class="token punctuation">(</span>drop<span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> grid_size <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>grid_norm <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span>dim<span class="token punctuation">)</span>            self<span class="token punctuation">.</span>avg_pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>ds_ratio<span class="token punctuation">,</span> stride<span class="token operator">=</span>ds_ratio<span class="token punctuation">)</span>            self<span class="token punctuation">.</span>ds_norm <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span>dim<span class="token punctuation">)</span>            self<span class="token punctuation">.</span>q <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>            self<span class="token punctuation">.</span>kv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> dim <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        B<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W <span class="token operator">=</span> x<span class="token punctuation">.</span>shape        qkv <span class="token operator">=</span> self<span class="token punctuation">.</span>qkv<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>grid_size <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>            grid_h<span class="token punctuation">,</span> grid_w <span class="token operator">=</span> H <span class="token operator">//</span> self<span class="token punctuation">.</span>grid_size<span class="token punctuation">,</span> W <span class="token operator">//</span> self<span class="token punctuation">.</span>grid_size            qkv <span class="token operator">=</span> qkv<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> grid_h<span class="token punctuation">,</span> self<span class="token punctuation">.</span>grid_size<span class="token punctuation">,</span> grid_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>grid_size<span class="token punctuation">)</span>            qkv <span class="token operator">=</span> qkv<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>            qkv <span class="token operator">=</span> qkv<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>grid_size <span class="token operator">*</span> self<span class="token punctuation">.</span>grid_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>            q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v <span class="token operator">=</span> qkv<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> qkv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> qkv<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>            attn <span class="token operator">=</span> <span class="token punctuation">(</span>q @ k<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>scale            attn <span class="token operator">=</span> attn<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>            grid_x <span class="token operator">=</span> <span class="token punctuation">(</span>attn @ v<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> grid_h<span class="token punctuation">,</span> grid_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>grid_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>grid_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>            grid_x <span class="token operator">=</span> grid_x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span>            grid_x <span class="token operator">=</span> self<span class="token punctuation">.</span>grid_norm<span class="token punctuation">(</span>x <span class="token operator">+</span> grid_x<span class="token punctuation">)</span>            q <span class="token operator">=</span> self<span class="token punctuation">.</span>q<span class="token punctuation">(</span>grid_x<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>            q <span class="token operator">=</span> q<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>            kv <span class="token operator">=</span> self<span class="token punctuation">.</span>kv<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ds_norm<span class="token punctuation">(</span>self<span class="token punctuation">.</span>avg_pool<span class="token punctuation">(</span>grid_x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            kv <span class="token operator">=</span> self<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>            kv <span class="token operator">=</span> kv<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>            k<span class="token punctuation">,</span> v <span class="token operator">=</span> kv<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            qkv <span class="token operator">=</span> qkv<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>            qkv <span class="token operator">=</span> qkv<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>            q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v <span class="token operator">=</span> qkv<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> qkv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> qkv<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>        attn <span class="token operator">=</span> <span class="token punctuation">(</span>q @ k<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>scale        attn <span class="token operator">=</span> attn<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        global_x <span class="token operator">=</span> <span class="token punctuation">(</span>attn @ v<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>grid_size <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>            global_x <span class="token operator">=</span> global_x <span class="token operator">+</span> grid_x        x <span class="token operator">=</span> self<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>global_x<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> x      <span class="token comment"># IRB模块</span><span class="token keyword">class</span> <span class="token class-name">InvertedResidual</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_dim<span class="token punctuation">,</span> hidden_dim<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> out_dim<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>                 drop<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">,</span> act_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>SiLU<span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>InvertedResidual<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        hidden_dim <span class="token operator">=</span> hidden_dim <span class="token keyword">or</span> in_dim        out_dim <span class="token operator">=</span> out_dim <span class="token keyword">or</span> in_dim        pad <span class="token operator">=</span> <span class="token punctuation">(</span>kernel_size <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            norm_layer<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>            act_layer<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span>kernel_size<span class="token punctuation">,</span> padding<span class="token operator">=</span>pad<span class="token punctuation">,</span> groups<span class="token operator">=</span>hidden_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            norm_layer<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>            act_layer<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> out_dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            norm_layer<span class="token punctuation">(</span>out_dim<span class="token punctuation">)</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout2d<span class="token punctuation">(</span>drop<span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x<span class="token comment"># 将二者进行组合的模块</span><span class="token keyword">class</span> <span class="token class-name">Block</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> head_dim<span class="token punctuation">,</span> grid_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ds_ratio<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> expansion<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>                 drop<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">,</span> drop_path<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> act_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>SiLU<span class="token punctuation">,</span>                 norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Block<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>drop_path <span class="token operator">=</span> DropPath<span class="token punctuation">(</span>drop_path<span class="token punctuation">)</span> <span class="token keyword">if</span> drop_path <span class="token operator">></span> <span class="token number">0.</span> <span class="token keyword">else</span> nn<span class="token punctuation">.</span>Identity<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>attn <span class="token operator">=</span> Attention<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> head_dim<span class="token punctuation">,</span> grid_size<span class="token operator">=</span>grid_size<span class="token punctuation">,</span> ds_ratio<span class="token operator">=</span>ds_ratio<span class="token punctuation">,</span>            drop<span class="token operator">=</span>drop<span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> InvertedResidual<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> hidden_dim<span class="token operator">=</span>dim <span class="token operator">*</span> expansion<span class="token punctuation">,</span> out_dim<span class="token operator">=</span>dim<span class="token punctuation">,</span>            kernel_size<span class="token operator">=</span>kernel_size<span class="token punctuation">,</span> drop<span class="token operator">=</span>drop<span class="token punctuation">,</span> act_layer<span class="token operator">=</span>act_layer<span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>drop_path<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attn<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>drop_path<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后是TDB下采样模块。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Downsample</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_dim<span class="token punctuation">,</span> out_dim<span class="token punctuation">,</span> act_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>SiLU<span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Downsample<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_dim<span class="token punctuation">,</span> out_dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>residual <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_dim<span class="token punctuation">,</span> out_dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>norm1 <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span>out_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>norm2 <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span>out_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>act <span class="token operator">=</span> act_layer<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x1 <span class="token operator">=</span> self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x2 <span class="token operator">=</span> self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>residual<span class="token punctuation">(</span>self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>act<span class="token punctuation">(</span>x1 <span class="token operator">+</span> x2<span class="token punctuation">)</span>        <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></br><p>TransCNN模型设计了两种网络结构，两种结构只在模块堆叠的数量上由区别，整体的参数表如下图所示：</p><p><a href="https://imgtu.com/i/LfywTO"><img src="https://s1.ax1x.com/2022/04/23/LfywTO.png" alt="LfywTO.png"></a></p><p>其中<code>C</code>表示通道数，<code>S</code>表示步长，<code>K</code>表示卷积核的大小，<code>E</code>表示IRB模块中的扩展率。在完整的TransCNN网络中，会先使用两个卷积对输入的图像进行降维，获得1&#x2F;4×1&#x2F;4大小的特征图进行后续操作。并在最后的预测时，使用全局平均池化和全连接层来输出可能的类别数，并使用Softmax获得最有可能的预测类别。</p></br><h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h3><p>该论文提出了一种结合了CNN和Transformer优势的混合模型TransCNN，并且提出了一个层级多头注意力机制模块（H-MHSA)用于减少原始Transformer中计算注意力机制导致的空间复杂度过高的问题。该模型在多个数据集上进行测试，获得了很好的实验结果。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;1-文献出处&quot;&gt;&lt;a href=&quot;#1-文献出处&quot; class=&quot;headerlink&quot; title=&quot;1.文献出处&quot;&gt;&lt;/a&gt;1.文献出处&lt;/h3&gt;&lt;p&gt;文献名：&lt;em&gt;Transformer in Convolutional Neural Networks&lt;/</summary>
      
    
    
    
    <category term="论文阅读" scheme="https://zhahoi.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Transformer CNN" scheme="https://zhahoi.github.io/tags/Transformer-CNN/"/>
    
  </entry>
  
  <entry>
    <title>使用Git上传代码到Github</title>
    <link href="https://zhahoi.github.io/posts/54abec2e.html"/>
    <id>https://zhahoi.github.io/posts/54abec2e.html</id>
    <published>2022-04-22T06:18:43.000Z</published>
    <updated>2022-04-23T12:13:03.091Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-git全局配置"><a href="#1-git全局配置" class="headerlink" title="1.git全局配置"></a>1.git全局配置</h3><pre class="line-numbers language-none"><code class="language-none">git config --global user.name &quot;your user name&quot;git config --global user.email &quot;your email&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="2-创建新的版本库"><a href="#2-创建新的版本库" class="headerlink" title="2.创建新的版本库"></a>2.创建新的版本库</h3><pre class="line-numbers language-none"><code class="language-none">git clone git@***.***.***.gitcd existing_foldertouch README.mdgit add README.mdgit commit -m &quot;add README&quot;git push -u origin main&#x2F;master<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-针对已经存在的文件夹"><a href="#3-针对已经存在的文件夹" class="headerlink" title="3.针对已经存在的文件夹"></a>3.针对已经存在的文件夹</h3><pre class="line-numbers language-none"><code class="language-none">cd existing_folder# 对文件夹进行git初始化git init# 将本地仓库与远方仓库进行连接git remote add origin git@***.***.***.git# 如果远端已经有了文件就先拉一下git pull --rebase origin main# 将修改的文件进行添加git add .git commit -m &quot;Initial commit&quot;git push -u origin main<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-更新已经存在文件夹中的内容"><a href="#4-更新已经存在文件夹中的内容" class="headerlink" title="4.更新已经存在文件夹中的内容"></a>4.更新已经存在文件夹中的内容</h3><pre class="line-numbers language-none"><code class="language-none"># 将修改的文件进行添加git add &quot;modify file&quot;git commit -m &quot;Initial commit&quot;git push -u origin main<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="5-删除Github仓库中的某个文件或者分支"><a href="#5-删除Github仓库中的某个文件或者分支" class="headerlink" title="5.删除Github仓库中的某个文件或者分支"></a>5.删除Github仓库中的某个文件或者分支</h3><ul><li>远程删除分支</li></ul><pre class="line-numbers language-none"><code class="language-none"># 查看所有分支git brach -a# 如果需要删除某个分支，就必须先切换到其他的分支上，如切换到master分支上git checkout master# 删除远程分支，如需要删除的远程分支为new_agit push origin --delete new_a<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>删除文件夹或者仓库</li></ul><pre class="line-numbers language-none"><code class="language-none"># 先将远程仓库中的内容拉下来git pull origin main# 查看拉下来有哪些文件dir# 删除某个文件或者文件夹，如targetgit rm -r --cached target # 提交删除说明git commit -m &#39;delete target file&#39;# 将修改的内容上传到远端git push -u origin main<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;1-git全局配置&quot;&gt;&lt;a href=&quot;#1-git全局配置&quot; class=&quot;headerlink&quot; title=&quot;1.git全局配置&quot;&gt;&lt;/a&gt;1.git全局配置&lt;/h3&gt;&lt;pre class=&quot;line-numbers language-none&quot;&gt;&lt;code</summary>
      
    
    
    
    <category term="常用指令" scheme="https://zhahoi.github.io/categories/%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/"/>
    
    
    <category term="git" scheme="https://zhahoi.github.io/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://zhahoi.github.io/posts/4a17b156.html"/>
    <id>https://zhahoi.github.io/posts/4a17b156.html</id>
    <published>2022-04-22T04:32:59.741Z</published>
    <updated>2022-04-22T05:33:52.726Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
