<!DOCTYPE html><html lang="zh-CN"><head><meta name="baidu-site-verification" content="h7QqekqYfg"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Every man is the master of his own fortune."><meta name="keywords" content="hayes"><link rel="stylesheet" type="text/css" href="//fonts.loli.net/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="../css/style.css?v=2.0.5"><link rel="stylesheet" type="text/css" href="../css/highlight.css?v=2.0.5"><link rel="Shortcut Icon" href="../favicon.ico"><link rel="bookmark" href="../favicon.ico"><link rel="apple-touch-icon" href="../apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="../apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="../atom.xml"><title>Transformer in Convolutional Neural Networks论文阅读笔记 | 海因斯的部落格</title><meta name="generator" content="Hexo 5.4.2"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Transformer in Convolutional Neural Networks论文阅读笔记</h1><a id="logo" href="../.">海因斯的部落格</a><p class="description">做颗星星，有棱有角，还会发光。</p></div><div id="nav-menu"><a href="../." class="current"><i class="fa fa-home"> 首页</i></a><a href="../archives/"><i class="fa fa-archive"> 归档</i></a><a href="../about/"><i class="fa fa-user"> 关于</i></a><a href="../atom.xml"><i class="fa fa-rss"> 订阅</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="搜索"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">Transformer in Convolutional Neural Networks论文阅读笔记</h1><div class="post-meta"><a href="#comments" class="comment-count"></a><p><span class="date">Apr 23, 2022</span><span><a href="../categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" class="category">论文阅读</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><h3 id="1-文献出处"><a href="#1-文献出处" class="headerlink" title="1.文献出处"></a>1.文献出处</h3><p>文献名：<em>Transformer in Convolutional Neural Networks</em></p>
<p>论文地址:<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.03180">https://arxiv.org/abs/2106.03180</a> </p>
<p>代码地址：<a target="_blank" rel="noopener" href="https://github.com/yun-liu/TransCNN">https://github.com/yun-liu/TransCNN</a></p>
</br>

<h3 id="2-文献研究内容"><a href="#2-文献研究内容" class="headerlink" title="2.文献研究内容"></a>2.文献研究内容</h3><p>针对多头自注意力模块（Multi-Head Self-Attention, MHSA）中计算和空间复杂度过高导致Vision Transformer效率低下的缺陷，提出一种层级多头注意力机制模块（Hierarchical Multi-Head Self-Attention, H-MHSA)，最后搭建而成的模型被称为TransCNN。</p>
</br>

<h3 id="3-研究创新点"><a href="#3-研究创新点" class="headerlink" title="3.研究创新点"></a>3.研究创新点</h3><ul>
<li><p>提出了层级多头注意力机制模块（MHSA)来使得Transformer中的自注意力机制计算得更加灵活和高效。</p>
</li>
<li><p>不再计算所有tokens的注意力，而是将patches进一步分组到小网格(grids)中，并计算每个网格中的注意力。这一步捕捉了局部关系，并产生了更具辨别力的局部表示。然后将这些小网格合并成更大的网格，并通过将前一步中的小网格视为标记来计算每个新网格中的注意力。通过这种方式，模型基本上捕获了更大区域中的特征关系。该过程被迭代以逐渐减少tokens的数量。</p>
</li>
<li><p>在整个过程中，H-MHSA模块逐步计算不断增加的区域大小中的自注意力，并自然地以分层方式对全局关系进行建模。 由于每一步的每个网格只有少量的tokens，可以显着降低视觉变换器的计算&#x2F;空间复杂度。</p>
</li>
<li><p>与之前对序列数据进行操作的Transformer网络不同，TransCNN 直接处理 3D 特征图，因此与过去十年提出的先进 CNN 技术兼容。 TransCNN 本质上继承了 CNN 和Transformer的优点，因此在学习尺度&#x2F;移位不变的特征表示和对输入数据中的长期依赖建模方面表现良好。</p>
</br></li>
</ul>
<h3 id="4-研究思路与方法"><a href="#4-研究思路与方法" class="headerlink" title="4.研究思路与方法"></a>4.研究思路与方法</h3><p><img src="https://s3.bmp.ovh/imgs/2022/04/23/9104476ebb0d9010.png"></p>
<p>提出的TransCNN模型是由层级多头注意力机制模块（H-MHSA)、反向残差瓶颈模块（IRB)和双分支下采样模块（TDB)共同组成的，H-MHSA模块和IRB模块用于获得输入图像得局部与全局特征，TDB模块用于降低特征图的大小。这三个模块可以共同组成一个完整的特征提取网络。</p>
<p>H-MHSA模块、IRB模块和TDB模块的代码实现部分如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#H-MHSA模块</span>
<span class="token keyword">class</span> <span class="token class-name">Attention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> head_dim<span class="token punctuation">,</span> grid_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ds_ratio<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> drop<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Attention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">assert</span> dim <span class="token operator">%</span> head_dim <span class="token operator">==</span> <span class="token number">0</span>
        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> dim <span class="token operator">//</span> head_dim
        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> head_dim
        self<span class="token punctuation">.</span>scale <span class="token operator">=</span> self<span class="token punctuation">.</span>head_dim <span class="token operator">**</span> <span class="token operator">-</span><span class="token number">0.5</span>
        self<span class="token punctuation">.</span>grid_size <span class="token operator">=</span> grid_size

        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span>dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>qkv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> dim <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout2d<span class="token punctuation">(</span>drop<span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> grid_size <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>grid_norm <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span>dim<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>avg_pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>ds_ratio<span class="token punctuation">,</span> stride<span class="token operator">=</span>ds_ratio<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>ds_norm <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span>dim<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>q <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>kv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> dim <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        B<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
        qkv <span class="token operator">=</span> self<span class="token punctuation">.</span>qkv<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>grid_size <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>
            grid_h<span class="token punctuation">,</span> grid_w <span class="token operator">=</span> H <span class="token operator">//</span> self<span class="token punctuation">.</span>grid_size<span class="token punctuation">,</span> W <span class="token operator">//</span> self<span class="token punctuation">.</span>grid_size
            qkv <span class="token operator">=</span> qkv<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> grid_h<span class="token punctuation">,</span> self<span class="token punctuation">.</span>grid_size<span class="token punctuation">,</span> grid_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>grid_size<span class="token punctuation">)</span>
            qkv <span class="token operator">=</span> qkv<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
            qkv <span class="token operator">=</span> qkv<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>grid_size <span class="token operator">*</span> self<span class="token punctuation">.</span>grid_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>
            q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v <span class="token operator">=</span> qkv<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> qkv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> qkv<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>

            attn <span class="token operator">=</span> <span class="token punctuation">(</span>q @ k<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>scale
            attn <span class="token operator">=</span> attn<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            grid_x <span class="token operator">=</span> <span class="token punctuation">(</span>attn @ v<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> grid_h<span class="token punctuation">,</span> grid_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>grid_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>grid_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>
            grid_x <span class="token operator">=</span> grid_x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span>
            grid_x <span class="token operator">=</span> self<span class="token punctuation">.</span>grid_norm<span class="token punctuation">(</span>x <span class="token operator">+</span> grid_x<span class="token punctuation">)</span>

            q <span class="token operator">=</span> self<span class="token punctuation">.</span>q<span class="token punctuation">(</span>grid_x<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            q <span class="token operator">=</span> q<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            kv <span class="token operator">=</span> self<span class="token punctuation">.</span>kv<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ds_norm<span class="token punctuation">(</span>self<span class="token punctuation">.</span>avg_pool<span class="token punctuation">(</span>grid_x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            kv <span class="token operator">=</span> self<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            kv <span class="token operator">=</span> kv<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
            k<span class="token punctuation">,</span> v <span class="token operator">=</span> kv<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>

        <span class="token keyword">else</span><span class="token punctuation">:</span>
            qkv <span class="token operator">=</span> qkv<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            qkv <span class="token operator">=</span> qkv<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
            q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v <span class="token operator">=</span> qkv<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> qkv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> qkv<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>

        attn <span class="token operator">=</span> <span class="token punctuation">(</span>q @ k<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>scale
        attn <span class="token operator">=</span> attn<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        global_x <span class="token operator">=</span> <span class="token punctuation">(</span>attn @ v<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>grid_size <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>
            global_x <span class="token operator">=</span> global_x <span class="token operator">+</span> grid_x
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>global_x<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> x
      
<span class="token comment"># IRB模块</span>
<span class="token keyword">class</span> <span class="token class-name">InvertedResidual</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_dim<span class="token punctuation">,</span> hidden_dim<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> out_dim<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>
                 drop<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">,</span> act_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>SiLU<span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>InvertedResidual<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        hidden_dim <span class="token operator">=</span> hidden_dim <span class="token keyword">or</span> in_dim
        out_dim <span class="token operator">=</span> out_dim <span class="token keyword">or</span> in_dim
        pad <span class="token operator">=</span> <span class="token punctuation">(</span>kernel_size <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span>

        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            norm_layer<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>
            act_layer<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span>kernel_size<span class="token punctuation">,</span> padding<span class="token operator">=</span>pad<span class="token punctuation">,</span> groups<span class="token operator">=</span>hidden_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            norm_layer<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>
            act_layer<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> out_dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            norm_layer<span class="token punctuation">(</span>out_dim<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout2d<span class="token punctuation">(</span>drop<span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token keyword">return</span> x

<span class="token comment"># 将二者进行组合的模块</span>
<span class="token keyword">class</span> <span class="token class-name">Block</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> head_dim<span class="token punctuation">,</span> grid_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ds_ratio<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> expansion<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>
                 drop<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">,</span> drop_path<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> act_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>SiLU<span class="token punctuation">,</span>
                 norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Block<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>drop_path <span class="token operator">=</span> DropPath<span class="token punctuation">(</span>drop_path<span class="token punctuation">)</span> <span class="token keyword">if</span> drop_path <span class="token operator">></span> <span class="token number">0.</span> <span class="token keyword">else</span> nn<span class="token punctuation">.</span>Identity<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attn <span class="token operator">=</span> Attention<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> head_dim<span class="token punctuation">,</span> grid_size<span class="token operator">=</span>grid_size<span class="token punctuation">,</span> ds_ratio<span class="token operator">=</span>ds_ratio<span class="token punctuation">,</span>
            drop<span class="token operator">=</span>drop<span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> InvertedResidual<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> hidden_dim<span class="token operator">=</span>dim <span class="token operator">*</span> expansion<span class="token punctuation">,</span> out_dim<span class="token operator">=</span>dim<span class="token punctuation">,</span>
            kernel_size<span class="token operator">=</span>kernel_size<span class="token punctuation">,</span> drop<span class="token operator">=</span>drop<span class="token punctuation">,</span> act_layer<span class="token operator">=</span>act_layer<span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>drop_path<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attn<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>drop_path<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>最后是TDB下采样模块。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Downsample</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_dim<span class="token punctuation">,</span> out_dim<span class="token punctuation">,</span> act_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>SiLU<span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Downsample<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_dim<span class="token punctuation">,</span> out_dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>residual <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_dim<span class="token punctuation">,</span> out_dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm1 <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span>out_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm2 <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span>out_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>act <span class="token operator">=</span> act_layer<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x1 <span class="token operator">=</span> self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x2 <span class="token operator">=</span> self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>residual<span class="token punctuation">(</span>self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>act<span class="token punctuation">(</span>x1 <span class="token operator">+</span> x2<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

</br>

<p>TransCNN模型设计了两种网络结构，两种结构只在模块堆叠的数量上由区别，整体的参数表如下图所示：</p>
<p><a target="_blank" rel="noopener" href="https://imgtu.com/i/LfywTO"><img src="https://s1.ax1x.com/2022/04/23/LfywTO.png" alt="LfywTO.png"></a></p>
<p>其中<code>C</code>表示通道数，<code>S</code>表示步长，<code>K</code>表示卷积核的大小，<code>E</code>表示IRB模块中的扩展率。在完整的TransCNN网络中，会先使用两个卷积对输入的图像进行降维，获得1&#x2F;4×1&#x2F;4大小的特征图进行后续操作。并在最后的预测时，使用全局平均池化和全连接层来输出可能的类别数，并使用Softmax获得最有可能的预测类别。</p>
</br>

<h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h3><p>该论文提出了一种结合了CNN和Transformer优势的混合模型TransCNN，并且提出了一个层级多头注意力机制模块（H-MHSA)用于减少原始Transformer中计算注意力机制导致的空间复杂度过高的问题。该模型在多个数据集上进行测试，获得了很好的实验结果。</p>
</div><div class="post-copyright"><blockquote><p>原文作者: zhahoi</p><p>原文链接: <a href="https://zhahoi.github.io/posts/9756efb3.html">https://zhahoi.github.io/posts/9756efb3.html</a></p><p>版权声明: 转载请注明出处(必须保留原文作者署名原文链接)</p></blockquote></div><div class="tags"><a href="../tags/Transformer-CNN/">Transformer CNN</a></div><div class="post-share"><div class="social-share"><span>分享到:</span></div></div><div class="post-nav"><a href="54abec2e.html" class="next">使用Git上传代码到Github</a></div><div id="comments"><div id="lv-container" data-id="city" data-uid="MTAyMC80OTA0MS8yNTUzNg=="></div></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%96%87%E7%8C%AE%E5%87%BA%E5%A4%84"><span class="toc-number">1.</span> <span class="toc-text">1.文献出处</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%96%87%E7%8C%AE%E7%A0%94%E7%A9%B6%E5%86%85%E5%AE%B9"><span class="toc-number">2.</span> <span class="toc-text">2.文献研究内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E7%A0%94%E7%A9%B6%E5%88%9B%E6%96%B0%E7%82%B9"><span class="toc-number">3.</span> <span class="toc-text">3.研究创新点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E7%A0%94%E7%A9%B6%E6%80%9D%E8%B7%AF%E4%B8%8E%E6%96%B9%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">4.研究思路与方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E6%80%BB%E7%BB%93"><span class="toc-number">5.</span> <span class="toc-text">5.总结</span></a></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="">Transformer in Convolutional Neural Networks论文阅读笔记</a></li><li class="post-list-item"><a class="post-list-link" href="54abec2e.html">使用Git上传代码到Github</a></li><li class="post-list-item"><a class="post-list-link" href="4a17b156.html">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="../categories/%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/">常用指令</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><span class="category-list-count">1</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="../tags/Transformer-CNN/" style="font-size: 15px;">Transformer CNN</a> <a href="../tags/git/" style="font-size: 15px;">git</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="../archives/2022/">2022</a><span class="archive-list-count">3</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-you"> 友情链接</i></div><ul></ul><a href="https://www.pyimagesearch.com/" title="pyimagesearch" target="_blank">pyimagesearch</a><ul></ul><a href="https://blog.floydhub.com/" title="floydhub's blog" target="_blank">floydhub's blog</a><ul></ul><a href="https://keras-cn.readthedocs.io/" title="Keras中文文档" target="_blank">Keras中文文档</a></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">网站地图</a> |  <a href="/atom.xml">订阅本站</a> |  <a href="/about/">联系博主</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次，本站总访客数:<i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>人</p><p><span> Copyright &copy;<a href="../." rel="nofollow">zhahoi.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a target="_blank" rel="noopener" href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','164591113','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?b803f369eae2203f4e666f3d6f7c5e01";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="../js/search.json.js?v=2.0.5"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML" async></script><div id="fullscreen-img" class="hide"><span class="close"></span></div><script type="text/javascript" src="../js/imgview.js?v=2.0.5" async></script><script type="text/javascript" src="../js/toctotop.js?v=2.0.5" async></script><link rel="stylesheet" type="text/css" href="../share/css/share.css"><script type="text/javascript" src="../share/js/social-share.js" charset="utf-8"></script><script type="text/javascript" src="../share/js/qrcode.js" charset="utf-8"></script><script>(function(d, s) {
  var j, e = d.getElementsByTagName('body')[0];
  if (typeof LivereTower === 'function') { return; }
  j = d.createElement(s);
  j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
  j.async = true;
  e.appendChild(j);
})(document, 'script');
</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/chitose.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":1}});</script></body></html>